package com.hotstar.datamodel.streaming.spark.processor;

import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.TimeUnit;

import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.sql.DataFrame;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.hive.HiveContext;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;

import com.mobileum.common.logger.ILogger;
import com.mobileum.common.logger.LoggerService;
import com.mobileum.streaming.riutil.CapFieldsEnum;
import com.mobileum.streaming.riutil.IsupFieldsEnum;
import com.mobileum.streaming.riutil.MLineRecordReaderWrapper;
import com.mobileum.streaming.riutil.MapFieldsEnum;
import com.mobileum.streaming.riutil.sparkUtil;

import scala.Tuple2;

public class RIsDataModelProcessor implements IStreamingProcessor<Object[]>  {
	
	private static final long serialVersionUID = 2127481946018247887L;
	private static ILogger logger = LoggerService.getLogger(MLineRecordReaderWrapper.class.getName());
	private long startTime;
	//private JavaRDD<Object[]> mapSignallingstats;
	private long dataCount;
	private JavaRDD<Row> isupDataRDD;
	private JavaRDD<Row> capDataRDD;
	private JavaRDD<Row> mapDataRDD;
	private StructType mapSchema;
	private StructType capSchema;
	private StructType isupSchema;
	private HiveContext hiveContext;
	
	@Override
	public void processData(JavaRDD<Object[]> data) {
		
		dataCount = data.count();
		//logger.warn("##################### count of input:"+ dataCount);
		
		if(dataCount == 0)
			return;
		
		splitSS7Data(data);
		hiveContext = sparkUtil.getHiveContext(data);
		
		createSchema();
		
		// Apply the schema to the RDD.
		DataFrame mapDataFrame = hiveContext.createDataFrame(mapDataRDD, mapSchema);
		
		DataFrame capDataFrame = hiveContext.createDataFrame(capDataRDD, capSchema);
		
		DataFrame isupDataFrame = hiveContext.createDataFrame(isupDataRDD, isupSchema);
		
		enhanceData(mapDataFrame, capDataFrame, isupDataFrame);
		
		processMapData(mapDataFrame);	
		processSMSDataModel(mapDataFrame, capDataFrame);
		processVoiceDataModel(isupDataFrame, capDataFrame);
	}

	private void enhanceData(DataFrame mapDataFrame, DataFrame capDataFrame, DataFrame isupDataFrame) {
		mapDataFrame.registerTempTable("ri_map_summary_data_tmp");
		
		 //mapSignallingDataFrame.printSchema();
		
		DataFrame enhanced_map_input_cos = hiveContext.sql(
				"SELECT ri_map_summary_data_tmp.*, nvl(ri_imsi_cos.cos_type, 0) cos_type, nvl(ri_imsi_cos.cos_id, 0) cos_id "
				+ "FROM ri_map_summary_data_tmp LEFT OUTER JOIN ri_imsi_cos "
				+ "ON  ri_map_summary_data_tmp."+MapFieldsEnum.MAPMESG_IMSI.getName()+" = ri_imsi_cos.imsi ");
		enhanced_map_input_cos.registerTempTable("enhanced_map_input_cos");
		DataFrame enhanced_map_summary_data = hiveContext.sql(
				"SELECT enhanced_map_input_cos.*, nvl(ri_imsi_device.imei/100000000,0) device_tac, "
				+ "nvl(ri_imsi_device.imei % 100000000, 0) device_svn, 1 node_type, mscVlr node_addr, "
				+ "to_date("+MapFieldsEnum.EVENT_DATETIME.getName()+") event_date "
				+ "FROM enhanced_map_input_cos LEFT OUTER JOIN ri_imsi_device "
				+ "ON enhanced_map_input_cos."+MapFieldsEnum.MAPMESG_IMSI.getName()+" = ri_imsi_device.imsi");

		
		enhanced_map_summary_data.registerTempTable("enhanced_map_summary_data_table");
		
		//enhanced_map_summary_data.limit(25).show();
		logger.warn("###################### Before Saving ri_map_esummary_data count: " + enhanced_map_summary_data.count()+printTimeDiffLogger());
		enhanced_map_summary_data.write().format("orc").partitionBy("event_date").mode("append")
				.saveAsTable("ri_map_esummary_data");
		
		logger.warn("###################### After Saving ri_map_esummary_data time: "+printTimeDiffLogger());
		
		
		//ISUP
		/*
		 * 2016-06-24 01:19:03.130 ERROR com.mobileum.streaming.ZeroMQDriver:
		 * 'streaming-job-executor-0': cannot resolve
		 * 'ri_isup_summary_data_tmp.Calling_Party' given input columns
		 * callingPartyNai, protocol, trunkcapacity, roamercalltype, day,
		 * anmpresent, carrierid, trunkgroupid, redirectingReason, date, seqno,
		 * callingParty, fileIdOfDetailFile, callednumberclass, eventType,
		 * calledParty, vlrZoneId, vvtype, acmTimestamp, anmTimestamp,
		 * callingPartyNpi, calledPartyNpi, eventFileId, transMediaReqd,
		 * callstatus, cos_type, genericNumber, carrierPc, ocn, calledPartyNai,
		 * rlcTimestamp, masterFileVersion, iamTimestamp, roamingPartnerNwId,
		 * hostNwId, calldirection, vlrNodeId, releaseCauseCode,
		 * congestionSeverity, releaseTimestamp, imsi, mypc, callid,
		 * callingnumberclass, chargeIndicator, eventFileOffset,
		 * redirectingNumber, cos_id, intpartnernwid, roamerType, sls,
		 * inter_HomeFlag, offset, calledMsisdn, cic; line 1 pos 168
		 * 
		 * 
		 * org.apache.spark.sql.AnalysisException: cannot resolve
		 * 'ri_isup_summary_data_tmp.Calling_Party' given input columns
		 * callingPartyNai, protocol, trunkcapacity, roamercalltype, day,
		 * anmpresent, carrierid, trunkgroupid, redirectingReason, date, seqno,
		 * callingParty, fileIdOfDetailFile, callednumberclass, eventType,
		 * calledParty, vlrZoneId, vvtype, acmTimestamp, anmTimestamp,
		 * callingPartyNpi, calledPartyNpi, eventFileId, transMediaReqd,
		 * callstatus, cos_type, genericNumber, carrierPc, ocn, calledPartyNai,
		 * rlcTimestamp, masterFileVersion, iamTimestamp, roamingPartnerNwId,
		 * hostNwId, calldirection, vlrNodeId, releaseCauseCode,
		 * congestionSeverity, releaseTimestamp, imsi, mypc, callid,
		 * callingnumberclass, chargeIndicator, eventFileOffset,
		 * redirectingNumber, cos_id, intpartnernwid, roamerType, sls,
		 * inter_HomeFlag, offset, calledMsisdn, cic; line 1 pos 168
		 */
		
		isupDataFrame.registerTempTable("ri_isup_summary_data_tmp");
		// -- map msisdn to imsi;
					DataFrame enhanced_isup_input_cos = hiveContext
					.sql("SELECT ri_isup_summary_data_tmp.*,nvl(ri_imsi_cos.cos_type, 0) cos_type, nvl(ri_imsi_cos.cos_id, 0) cos_id "
						+ "FROM ri_isup_summary_data_tmp LEFT OUTER JOIN ri_imsi_cos "
							+ "ON ri_isup_summary_data_tmp."+IsupFieldsEnum.CALLINGPARTY+" = ri_imsi_cos.imsi ")
					.coalesce(10);
					enhanced_isup_input_cos.registerTempTable("enhanced_isup_input_cos");
					DataFrame enhanced_isup_input_cos_dev = hiveContext
					.sql("SELECT enhanced_isup_input_cos.*, nvl(ri_imsi_device.imei/100000000,0) device_TAC, "
							+ "nvl(ri_imsi_device.imei % 100000000, 0) device_SVN, 1 node_type, "
							+ "cast("+IsupFieldsEnum.VLRNODEID+" as bigint) node_addr, to_date("+IsupFieldsEnum.DATE+") event_date "
						+ "FROM enhanced_isup_input_cos LEFT OUTER JOIN ri_imsi_device "
							+ "ON enhanced_isup_input_cos."+IsupFieldsEnum.CALLINGPARTY+" = ri_imsi_device.imsi")
					.coalesce(4);

					
		enhanced_isup_input_cos_dev.registerTempTable("enhanced_isup_summary_data_table");
				
		//CAP
				
		capDataFrame.registerTempTable("ri_cap_summary_data_tmp");
					
		DataFrame enhanced_cap_input_cos = hiveContext
				.sql("SELECT ri_cap_summary_data_tmp.*,nvl(ri_imsi_cos.cos_type, 0) cos_type, nvl(ri_imsi_cos.cos_id, 0) cos_id  "
						+ "FROM ri_cap_summary_data_tmp LEFT OUTER JOIN ri_imsi_cos "
						+ "ON ri_cap_summary_data_tmp."+CapFieldsEnum.IMSI+" = ri_imsi_cos.imsi ")
				.coalesce(10);
		enhanced_cap_input_cos.registerTempTable("enhanced_cap_input_cos");
		DataFrame enhanced_cap_input_cos_dev = hiveContext.sql(
				"SELECT enhanced_cap_input_cos.*, nvl(ri_imsi_device.imei/100000000,0) device_tac, "
				+ "nvl(ri_imsi_device.imei % 100000000, 0) device_svn, 2 node_type, mscAddress node_addr, to_date(date) event_date "
				+ "FROM enhanced_cap_input_cos LEFT OUTER JOIN ri_imsi_device "
				+ "ON enhanced_cap_input_cos."+CapFieldsEnum.IMSI+" = ri_imsi_device.imsi");

		enhanced_cap_input_cos_dev.registerTempTable("enhanced_cap_summary_data_table");			
		
	}

	private void processMapData(DataFrame mapDataFrame) {
		
		
		startTime = System.currentTimeMillis();
		
		
		logger.warn("##################### count of input after DF:"+ mapDataFrame.count());
		
		
		
		DataFrame map_signalling_sql = hiveContext.sql(
				"select cast(substring("+MapFieldsEnum.EVENT_DATETIME.getName()+",0,13) as timestamp) event_date_str, "
				+ MapFieldsEnum.TXNINFO_HOSTNWID.getName() + " hostId,"
				+ MapFieldsEnum.TXNINFO_RMNGPARTNERNWID.getName() + " partnerId, "
				+ "nvl("+MapFieldsEnum.HOST_MCC.getName()+",0) Host_MCC, "
				+ "nvl("+MapFieldsEnum.HOST_MNC.getName()+",0) Host_MNC, "
				+ "nvl("+MapFieldsEnum.PARTNER_MCC.getName()+",0) Partner_MCC,"
				+ "nvl("+MapFieldsEnum.PARTNER_MNC.getName()+",0) Partner_MNC, "
				+ "nvl("+MapFieldsEnum.TXNINFO_ROAMERTYPE.getName()+",0) roamerType,"
				+ " nvl("+MapFieldsEnum.MSGTYPE.getName()+",0) msgtype, "
				+ "nvl("+MapFieldsEnum.ERRORCODE.getName()+",0) errorcode, "
				+ "nvl("+MapFieldsEnum.CAUSENW.getName()+",0) error_cause_nw, "
				+ "abs(diffUdf("+MapFieldsEnum.STARTTIME.getName()+", "+MapFieldsEnum.ENDTIME.getName()+")) latency, "
				+ MapFieldsEnum.ISINTERHOME.getName()+" isInterHome, cos_type, cos_id, device_tac, device_svn, "
				+ "node_type, node_addr  from enhanced_map_summary_data_table");

		map_signalling_sql.registerTempTable("map_signalling_temp_table");
		
	/*	logger.warn("###################### printing map_signalling_sql");
		map_signalling_sql.printSchema();
		map_signalling_sql.limit(25).show();
		*/
		
		DataFrame map_signalling_nw_hrly = hiveContext
				.sql("select event_date_str, hostId,partnerId, Host_MCC, Host_MNC, Partner_MCC, Partner_MNC, roamerType,"
						+ "msgtype, errorcode, error_cause_nw,0 cos_type, 0 cos_id, 0 device_tac, 0 device_svn, "
						+ "count(*) count,sum(latency) latency, to_date(event_date_str) event_date "
						+ "from map_signalling_temp_table "
						+ "group by event_date_str, hostId,partnerId, Host_MCC, Host_MNC, Partner_MCC, Partner_MNC, "
						+ "roamerType,msgtype, errorcode, error_cause_nw ")
				.coalesce(5);

	/*	logger.warn("###################### printing map_signalling_nw_hrly");
		
		map_signalling_nw_hrly.printSchema();
		map_signalling_nw_hrly.limit(25).show();*/
		
		//map_signalling_nw_hrly.limit(25).show();
		
		logger.warn("###################### Before Saving ri_mapsig_nw_hrly count: " + map_signalling_nw_hrly.count()+printTimeDiffLogger());
		
		map_signalling_nw_hrly.write().format("orc").mode("append").partitionBy("event_date")
				.saveAsTable("ri_mapsig_nw_hrly");
		
		logger.warn("###################### After Saving ri_mapsig_nw_hrly time: "+printTimeDiffLogger());

		// Map Signaling Node Hrly ;
		DataFrame map_signalling_node_hrly = hiveContext
				.sql("select event_date_str, hostId,partnerId, Host_MCC, Host_MNC, Partner_MCC, Partner_MNC, roamerType"
						+ ",msgtype, errorcode, error_cause_nw, cos_type, cos_id, device_tac, device_svn, node_type, "
						+ "node_addr, count(*) count,sum(latency) latency, to_date(event_date_str) event_date "
						+ "from map_signalling_temp_table "
						+ "group by event_date_str, hostId,partnerId, Host_MCC, Host_MNC, Partner_MCC, Partner_MNC, "
						+ "roamerType,msgtype, errorcode, error_cause_nw, cos_type, cos_id, device_tac, device_svn, "
						+ "node_type, node_addr ")
				.coalesce(4);

		//map_signalling_node_hrly.limit(25).show();
		logger.warn("###################### Before Saving ri_mapsig_node_hrly count: " + map_signalling_node_hrly.count()+printTimeDiffLogger());
		map_signalling_node_hrly.write().format("orc").mode("append").partitionBy("event_date")
				.saveAsTable("ri_mapsig_node_hrly");
		
		logger.warn("###################### After Saving ri_mapsig_node_hrly time: "+printTimeDiffLogger());
		//logger.warn("### Saving ri_mapsig_node_hrly with count:" + map_signalling_node_hrly.count());
		
		
		
		
					
	}
	
	private void processVoiceDataModel(DataFrame isupDataFrame, DataFrame capDataFrame) {
		///////////////////////// ISUP Data
		// Enhancing ISUP Data;
		
		//hiveContext.sql("CACHE TABLE ri_isup_summary_data_tmp");
		
		isupDataFrame.registerTempTable("ri_isup_summary_data_tmp");
		
		// -- map msisdn to imsi;
		DataFrame enhanced_isup_input_cos = hiveContext
		.sql("SELECT ri_isup_summary_data_tmp.*,nvl(ri_imsi_cos.cos_type, 0) cos_type, nvl(ri_imsi_cos.cos_id, 0) cos_id "
			+ "FROM ri_isup_summary_data_tmp LEFT OUTER JOIN ri_imsi_cos "
				+ "ON ri_isup_summary_data_tmp.Calling_Party = ri_imsi_cos.imsi ")
		.coalesce(10);
		enhanced_isup_input_cos.registerTempTable("enhanced_isup_input_cos");
		DataFrame enhanced_isup_input_cos_dev = hiveContext
		.sql("SELECT enhanced_isup_input_cos.*, nvl(ri_imsi_device.imei/100000000,0) device_TAC, "
				+ "nvl(ri_imsi_device.imei % 100000000, 0) device_SVN, 1 node_type, "
				+ "cast(VLR_Node_Id as bigint) node_addr, to_date(date) event_date "
			+ "FROM enhanced_isup_input_cos LEFT OUTER JOIN ri_imsi_device "
				+ "ON enhanced_isup_input_cos.Calling_Party = ri_imsi_device.imsi")
		.coalesce(4);
		
		enhanced_isup_input_cos_dev.registerTempTable("enhanced_isup_summary_data_table");
		;
		//hiveContext.sql("CACHE TABLE enhanced_isup_summary_data_table");
		//hiveContext.sql("UNCACHE TABLE ri_cap_summary_data");
		;
		enhanced_isup_input_cos_dev.write().mode("append").partitionBy("event_date")
		.saveAsTable("ri_isup_esummary_data");
		//logger.warn("### Saving ri_isup_esummary_data with count:" + enhanced_isup_input_cos_dev.count());
		
		;
		
		// **************************************************** Roaming Voice ;
		
		// CAP;
		hiveContext.sql("CREATE TEMPORARY FUNCTION processVoiceCAP AS 'com.roamware.app.r360.hiveUDF.VoiceCAPUDF'");
		// answerpresent,erbcsmCallStatus, connectelpstime, answerTimestamp,
		// releaseTimestamp, disconnectTimestamp, callStatus, releaseCauseCode;
		// CALLDURATION, CALLSTATUS, ISCALLDROPPED(Int0),
		// ISCALLATTMPTSUCCESSFUL(Int0), ISCALLRELINTENTIONAL(Int0);
		
		DataFrame cap_voice_sql = hiveContext.sql(
		"select cast(substring(date,0,13) as timestamp) event_date_str,hostNwId  hostId,partnerNwId  partnerId, 0 Host_MCC, "
		+ "0 Host_MNC, 0 Partner_MCC, 0 Partner_MNC, roamerType,  node_type, node_addr, cos_type, cos_Id, device_TAC, device_SVN,"
		+ " protocol, camelVersion version, callType,1 source, 0 revenue, callstatus, interhomeflag, releaseCauseCode errorCode,  "
		+ "processVoiceCAP(answerpresent,erbcsmCallStatus, connectelpstime, answerTimestamp, releaseTimestamp, disconnectTimestamp, "
		+ "callStatus, releaseCauseCode) As output from enhanced_cap_summary_data_table where protocol = 4 and eventType = 0 ");
		
		// ISUP;
		hiveContext.sql("CREATE TEMPORARY FUNCTION processVoiceISUP AS 'com.roamware.app.r360.hiveUDF.VoiceISUPUDF'");
		// answerpresent,erbcsmCallStatus, connectelpstime, answerTimestamp,
		// releaseTimestamp, disconnectTimestamp, callStatus, releaseCauseCode;
		// CALLDURATION, CALLSTATUS, ISCALLDROPPED(Int0),
		// ISCALLATTMPTSUCCESSFUL(Int0), ISCALLRELINTENTIONAL(Int0);
		
		;
		;
		DataFrame isup_voice_sql_tmp = hiveContext.sql(
		"select hostNwId  hostId,RmgPartnerNwId  partnerId,0 Host_MCC,0 Host_MNC,0 Partner_MCC,0 Partner_MNC, roamerType,  node_type, node_addr, cos_type, cos_Id, device_TAC, device_SVN, protocol, 0 version, roamercalltype callType,1 source, 0 revenue, interhomeflag, RelCauseCode errorCode, processVoiceISUP(callstatus,relCauseCode, iam_Timestamp,ACM_Timestamp , ANM_TimeStamp , REL_Timestamp , RLC_Timestamp) As output from enhanced_isup_summary_data_table where protocol = 2 and roamerType != 0 ");
		
		DataFrame isup_voice_sql = isup_voice_sql_tmp.withColumn("event_date_str",
		isup_voice_sql_tmp.col("output.EVENTDATE"));
		
		/*
		* DataFrame voice_columns =
		* cap_voice_sql.columns.toSet.intersect(isup_voice_sql.columns.toSet).
		* map(col).toSeq; DataFrame voice_union_data =
		* cap_voice_sql.select(voice_columns:
		* _*).unionAll(isup_voice_sql.select(voice_columns: _*));
		* 
		* 
		* DataFrame voice_sql_interHome =
		* voice_union_data.filter($"interhomeflag" !==
		* 0).withColumnRenamed("hostId",
		* "partnerId_new").withColumnRenamed("partnerId","hostId")
		* withColumnRenamed("partnerId_new","partnerId");
		* 
		* DataFrame voice_union_columns =
		* voice_union_data.columns.toSet.intersect(voice_sql_interHome.columns.
		* toSet).map(col).toSeq; DataFrame voice_data =
		* voice_union_data.select(voice_union_columns:
		* _*).unionAll(voice_sql_interHome.select(voice_union_columns:
		* _*)).coalesce(10);
		* 
		* voice_data.registerTempTable("voice_data");
		*/
		
		isup_voice_sql.registerTempTable("voice_data");
		
		DataFrame ri_callsig_node_hrly = hiveContext
		.sql("select event_date_str, hostId, partnerId, Host_MCC, Host_MNC, Partner_MCC, Partner_MNC, roamerType, "
				+ "node_type, node_addr, 0 cos_type, 0 cos_Id, 0 device_TAC, 0 device_SVN, protocol, version, "
				+ "callType, output.callstatus, output.iscalldropped, output.iscallatmptsuccessful, "
				+ "output.iscallreleaseintentional, source, sum(revenue) revenue, count(*) call_count,"
				+ "sum(output.callduration) callduration, sum(output.pdd) pdd, to_date(event_date_str) event_date "
				+ "from voice_data "
				+ "group by event_date_str, hostId, partnerId, host_mcc, host_mnc, partner_mcc, partner_mnc, "
				+ "roamerType, node_type, node_addr, protocol, version, callType, output.callstatus, "
				+ "output.iscalldropped, output.iscallatmptsuccessful, output.iscallreleaseintentional, source ")
		.coalesce(4);
		
		// DataFrame voice_hrly_final_result =
		// voice_hrly.withColumn("event_date",
		// to_date(voice_hrly.col("event_date_str")));
		
		ri_callsig_node_hrly.write().format("orc").mode("append").partitionBy("event_date")
		.saveAsTable("ri_callsig_node_hrly");
		//logger.warn("### Saving ri_callsig_node_hrly with count:" + ri_callsig_node_hrly.count());
		
		
		DataFrame ri_callsig_nw_hrly = hiveContext
		.sql("select event_date_str, hostId, partnerId, Host_MCC, Host_MNC, Partner_MCC, Partner_MNC, roamerType,  0 cos_type, 0 cos_Id, 0 device_TAC, 0 device_SVN, protocol, version, callType, output.callstatus, output.iscalldropped, output.iscallatmptsuccessful, output.iscallreleaseintentional, source, sum(revenue) revenue, count(*) call_count,sum(output.callduration) callduration, sum(output.pdd) pdd, to_date(event_date_str) event_date from voice_data group by event_date_str, hostId, partnerId, host_mcc, host_mnc, partner_mcc, partner_mnc, roamerType,  protocol, version, callType, output.callstatus, output.iscalldropped, output.iscallatmptsuccessful, output.iscallreleaseintentional, source ")
		.coalesce(4);
		
		// DataFrame voice_hrly_final_result =
		// voice_hrly.withColumn("event_date",
		// to_date(voice_hrly.col("event_date_str")));
		
		ri_callsig_nw_hrly.write().format("orc").mode("append").partitionBy("event_date")
		.saveAsTable("ri_callsig_nw_hrly");
		//logger.warn("### Saving ri_callsig_nw_hrly with count:" + ri_callsig_nw_hrly.count());
		
		DataFrame ri_callusage_node_hrly = hiveContext
		.sql("select event_date_str, hostId, partnerId, Host_MCC, Host_MNC, Partner_MCC, Partner_MNC, roamerType, node_type, node_addr, cos_type, cos_Id, device_TAC, device_SVN, protocol, version, callType, output.callstatus, output.iscalldropped, output.iscallatmptsuccessful, output.iscallreleaseintentional, source, sum(revenue) revenue, count(*) call_count,sum(output.callduration) callduration, sum(output.pdd) pdd, to_date(event_date_str) event_date from voice_data group by event_date_str, hostId, partnerId, host_mcc, host_mnc, partner_mcc, partner_mnc, roamerType, node_type, node_addr,cos_type, cos_Id, device_TAC, device_SVN, protocol, version, callType, output.callstatus, output.iscalldropped, output.iscallatmptsuccessful, output.iscallreleaseintentional, source ")
		.coalesce(4);
		
		ri_callusage_node_hrly.write().format("orc").mode("append").partitionBy("event_date")
		.saveAsTable("ri_callusage_node_hrly");
		//logger.warn("### Saving ri_callusage_node_hrly with count:" + ri_callusage_node_hrly.count());
		
		DataFrame ri_callusage_nw_hrly = hiveContext
		.sql("select event_date_str, hostId, partnerId, Host_MCC, Host_MNC, Partner_MCC, Partner_MNC, roamerType, cos_type, cos_Id, device_TAC, device_SVN, protocol, version, callType, output.callstatus, output.iscalldropped, output.iscallatmptsuccessful, output.iscallreleaseintentional, source, sum(revenue) revenue, count(*) call_count,sum(output.callduration) callduration, sum(output.pdd) pdd, to_date(event_date_str) event_date from voice_data group by event_date_str, hostId, partnerId, host_mcc, host_mnc, partner_mcc, partner_mnc, roamerType,cos_type, cos_Id, device_TAC, device_SVN, protocol, version, callType, output.callstatus, output.iscalldropped, output.iscallatmptsuccessful, output.iscallreleaseintentional, source ")
		.coalesce(4);
		
		ri_callusage_nw_hrly.write().format("orc").mode("append").partitionBy("event_date")
		.saveAsTable("ri_callusage_nw_hrly");
		//logger.warn("### Saving ri_callusage_nw_hrly with count:" + ri_callusage_nw_hrly.count());
		
		hiveContext.sql("UNCACHE TABLE ri_imsi_cos");
		hiveContext.sql("UNCACHE TABLE ri_imsi_device");

}

private void processSMSDataModel(DataFrame mapDataFrame, DataFrame capDataFrame) {
		

		/////////////// ************************************ Roaming SMS;

		// hiveContext.sql("CACHE TABLE enhanced_map_summary_data_table");
		// hiveContext.sql("CACHE TABLE ri_cap_esummary_data");

		// MAP;
		DataFrame roaming_sms_map_sql = hiveContext.sql(
			"select cast(substring("+MapFieldsEnum.EVENT_DATETIME.getName()+",0,13) as timestamp) event_date_str, "
					+ MapFieldsEnum.TXNINFO_HOSTNWID.getName() + " host_id, "
					+ MapFieldsEnum.TXNINFO_PARTNERNWID.getName() + " partner_id, "
					+ MapFieldsEnum.HOST_MCC.getName() +" host_mcc, "
					+ MapFieldsEnum.HOST_MNC.getName() + " host_mnc, "
					+ MapFieldsEnum.PARTNER_MCC.getName() +" partner_mcc, "
					+ MapFieldsEnum.PARTNER_MNC.getName() +" partner_mnc, "
					+ MapFieldsEnum.TXNINFO_ROAMERTYPE.getName() +" roamer_type, "
					+ "node_type, node_addr, cos_type, cos_Id, device_TAC, device_svn, "
					+ MapFieldsEnum.PROTOCOL.getName()+" protocol , "
					+ MapFieldsEnum.TXNINFO_SMSTYPE.getName() +" sms_type, 1 source, errorCode, "
					+ "(case "+MapFieldsEnum.ISTXNFAILED.getName() +" when 0 then 0 when 1 then "
						+ "((case "+MapFieldsEnum.ERRORCODE.getName() +" when 226 then 2 else 1 end)) "
								+ "else 2 end ) sms_status, "
					+ MapFieldsEnum.ISINTERHOME.getName()
				+ "from enhanced_map_summary_data_table "
				+ "where "+MapFieldsEnum.PROTOCOL.getName() +" = 3 and "
					+ MapFieldsEnum.MSGTYPE.getName() +" in (10,11)  and "
					+ "("+MapFieldsEnum.TXNINFO_ROAMERTYPE.getName() +" in (1,2))");

		// CAP;
		DataFrame roaming_sms_cap_sql = hiveContext.sql(
				"select cast(substring("+CapFieldsEnum.DATE.getName()+",0,13) as timestamp) event_date_str, "
					+ CapFieldsEnum.HOSTNWID.getName()+" host_id, "
					+ CapFieldsEnum.PARTNERNWID.getName()+" partner_id,"
					+ " 0  host_mcc, 0 host_mnc, 0 partner_mcc, 0 partner_mnc, "
					+ CapFieldsEnum.ROAMERTYPE.getName()+" roamer_type, node_type, node_addr, "
					+ "cos_type, cos_Id, device_TAC, device_svn, "
					+ CapFieldsEnum.PROTOCOL.getName()+", 1 sms_type, 1 source, "
					+ "(case "+CapFieldsEnum.RELEASECAUSECODE.getName()
							+ "when 700 then 0 when 0 then 2 else 1 end ) sms_status, "
					+ CapFieldsEnum.RELEASECAUSECODE.getName()+" errorCode, "
					+ CapFieldsEnum.INTERHOMEFLAG.getName()+" isInterHome "
				+ "from enhanced_cap_summary_data_table "
				+ "where "+CapFieldsEnum.PROTOCOL.getName()+" = 4 and "
					+ CapFieldsEnum.EVENTTYPE.getName()+" = 1 and "
					+ CapFieldsEnum.ROAMERTYPE.getName()+" in (1,2)");

		/*
		 * DataFrame sms_columns =
		 * roaming_sms_map_sql.columns().toSet.intersect(roaming_sms_cap_sql.
		 * columns().toSet).map(col).toSeq; DataFrame roaming_sms_union_data =
		 * roaming_sms_map_sql.select(sms_columns:
		 * _*).unionAll(roaming_sms_cap_sql.select(sms_columns: _*));
		 * 
		 * /* DataFrame roaming_sms_sql_interHome =
		 * roaming_sms_union_data.filter($"isInterHome" !==
		 * 0).withColumnRenamed("hostId",
		 * "partnerId_new").withColumnRenamed("partnerId","hostId")
		 * withColumnRenamed("partnerId_new","partnerId");
		 * 
		 * 
		 * DataFrame union_columns =
		 * roaming_sms_union_data.columns.toSet.intersect(
		 * roaming_sms_sql_interHome.columns.toSet).map(col).toSeq; DataFrame
		 * roaming_sms_data = roaming_sms_union_data.select(union_columns:
		 * _*).unionAll(roaming_sms_sql_interHome.select(union_columns:
		 * _*)).coalesce(10);
		 * 
		 * roaming_sms_data.registerTempTable("roaming_sms_temp_table");
		 */

		roaming_sms_map_sql.registerTempTable("roaming_sms_temp_table");

		hiveContext.sql("CACHE TABLE roaming_sms_temp_table");

		DataFrame ri_smsusage_node_hrly = hiveContext
				.sql("select event_date_str, host_id, partner_id, host_mcc, host_mnc, partner_mcc, partner_mnc, "
						+ "roamer_type, node_type, node_addr, cos_type, cos_Id, device_TAC, device_svn, protocol, "
						+ "sms_type, source, 1 revenue, sms_status, count(*) count, to_date(event_date_str) event_date "
					+ "from roaming_sms_temp_table "
					+ "group by event_date_str, host_id, partner_id, host_mcc, host_mnc, partner_mcc, partner_mnc, "
					+ "roamer_type, node_type, node_addr, cos_type, cos_Id, device_TAC, device_svn, protocol, sms_type, "
					+ "source, sms_status")
				.coalesce(4);

		;
		ri_smsusage_node_hrly.write().format("orc").mode("append").partitionBy("event_date")
				.saveAsTable("ri_smsusage_node_hrly");
		//logger.warn("### Saving ri_smsusage_node_hrly with count:" + ri_smsusage_node_hrly.count());
		DataFrame ri_smsusage_nw_hrly = hiveContext
				.sql("select event_date_str, host_id, partner_id, host_mcc, host_mnc, partner_mcc, partner_mnc, "
						+ "roamer_type, cos_type, cos_Id, device_TAC, device_svn, protocol, sms_type, source, 1 revenue, "
						+ "sms_status, count(*) count, to_date(event_date_str) event_date "
					+ "from roaming_sms_temp_table "
					+ "group by event_date_str, host_id, partner_id, host_mcc, host_mnc, partner_mcc, partner_mnc, "
						+ "roamer_type, node_type, node_addr, cos_type, cos_Id, device_TAC, device_svn, protocol, "
						+ "sms_type, source, sms_status")
				.coalesce(4);

		;
		ri_smsusage_nw_hrly.write().format("orc").mode("append").partitionBy("event_date")
				.saveAsTable("ri_smsusage_nw_hrly");
		//logger.warn("### Saving ri_smsusage_nw_hrly with count:" + ri_smsusage_nw_hrly.count());
		DataFrame ri_smssig_node_hrly = hiveContext
				.sql("select event_date_str, host_id, partner_id, host_mcc, host_mnc, partner_mcc, partner_mnc, "
						+ "roamer_type, node_type, node_addr, 0 cos_type, 0 cos_Id, 0 device_TAC, 0 device_svn, "
						+ "protocol, sms_type, source, sms_status, errorCode, count(*) count, "
						+ "to_date(event_date_str) event_date "
					+ "from roaming_sms_temp_table "
					+ "group by event_date_str, host_id, partner_id, host_mcc, host_mnc, partner_mcc, "
						+ "partner_mnc, roamer_type, node_type, node_addr,  protocol, sms_type, source, sms_status,"
						+ " errorCode")
				.coalesce(4);

		ri_smssig_node_hrly.write().format("orc").mode("append").partitionBy("event_date")
				.saveAsTable("ri_smssig_node_hrly");
		//logger.warn("### Saving ri_smssig_node_hrly with count:" + ri_smssig_node_hrly.count());
		DataFrame ri_smssig_nw_hrly = hiveContext
				.sql("select event_date_str, host_id, partner_id, host_mcc, host_mnc, partner_mcc, partner_mnc, "
						+ "roamer_type, 0 cos_type, 0 cos_Id, 0 device_TAC, 0 device_svn, protocol, sms_type, "
						+ "source,sms_status, errorCode, count(*) count, to_date(event_date_str) event_date "
					+ "from roaming_sms_temp_table "
					+ "group by event_date_str, host_id, partner_id, host_mcc, host_mnc, partner_mcc, partner_mnc, "
						+ "roamer_type, node_type, node_addr,  protocol, sms_type, source, sms_status, errorCode")
				.coalesce(4);

		ri_smssig_nw_hrly.write().format("orc").mode("append").partitionBy("event_date")
				.saveAsTable("ri_smssig_nw_hrly");
		//logger.warn("### Saving ri_smssig_nw_hrly with count:" + ri_smssig_nw_hrly.count());
		
		hiveContext.sql("UNCACHE TABLE enhanced_map_summary_data_table");
		hiveContext.dropTempTable("enhanced_map_summary_data_table");

		
		
	}
	
	
	
	
	
	
	
	
	
	
	
	private void createSchema() {
		List<StructField> mapFields = new ArrayList<StructField>();
		for ( MapFieldsEnum fieldName: MapFieldsEnum.values()) {
		  mapFields.add(DataTypes.createStructField(fieldName.getName(), fieldName.getDataType(), true));
		}
		mapSchema = DataTypes.createStructType(mapFields);
		
		List<StructField> capFields = new ArrayList<StructField>();
		for ( CapFieldsEnum fieldName: CapFieldsEnum.values()) {
			capFields.add(DataTypes.createStructField(fieldName.getName(), fieldName.getDataType(), true));
		}
		capSchema = DataTypes.createStructType(capFields);
		
		List<StructField> isupFields = new ArrayList<StructField>();
		for ( IsupFieldsEnum fieldName: IsupFieldsEnum.values()) {
			isupFields.add(DataTypes.createStructField(fieldName.getName(), fieldName.getDataType(), true));
		}
		isupSchema = DataTypes.createStructType(isupFields);
			
	}

	private void splitSS7Data(JavaRDD<Object[]> data) {
		JavaRDD<Tuple2<Integer, Object[]>> splittedData = data.map(new Function<Object[],Tuple2<Integer, Object[]>>()
	    {
	        private static final long serialVersionUID = -2381522520231963249L;

	        @Override
	        public Tuple2<Integer, Object[]> call(Object[] ss7Data) throws Exception
	        {
	            // Split a row of data by commas (,).
	          //  String[] tokens = s.split(",");

	            // Integrate the three split elements to a ternary Tuple.
	            Tuple2<Integer, Object[]> person = new Tuple2<Integer, Object[]>(Integer.parseInt(ss7Data[2].toString()), ss7Data);
	            return person;
	        }
	    });
		
		isupDataRDD = splittedData.filter(new Function<Tuple2<Integer, Object[]>, Boolean>()
	    {
	        private static final long serialVersionUID = -4210609503909770492L;

	        @Override
	        public Boolean call(Tuple2<Integer, Object[]> ss7Data) throws Exception
	        {
	            // Filter the records of which the sex in the second column is female.
	            Boolean isIsupData = (ss7Data._1() == 2);
	            return isIsupData;
	        }
	    }).map(
	   		  new Function<Tuple2<Integer, Object[]>, Row>() {//int tempCount=0;
		 		    private static final long serialVersionUID = 8554347338412767649L;

		 			public Row call(Tuple2<Integer, Object[]> record) throws Exception {		    	
		 		      return RowFactory.create(record._2());
		 		    }
		 		  });;
	    
	    capDataRDD = splittedData.filter(new Function<Tuple2<Integer, Object[]>, Boolean>()
	    {
	        private static final long serialVersionUID = -4210609503909770492L;

	        @Override
	        public Boolean call(Tuple2<Integer, Object[]> ss7Data) throws Exception
	        {
	            // Filter the records of which the sex in the second column is female.
	            Boolean isIsupData = (ss7Data._1() == 4);
	            return isIsupData;
	        }
	    }).map(
	   		  new Function<Tuple2<Integer, Object[]>, Row>() {//int tempCount=0;
		 		    private static final long serialVersionUID = 8554347338412767649L;

		 			public Row call(Tuple2<Integer, Object[]> record) throws Exception {		    	
		 		      return RowFactory.create(record._2());
		 		    }
		 		  });
	    
	    mapDataRDD = splittedData.filter(new Function<Tuple2<Integer, Object[]>, Boolean>()
	    {
	        private static final long serialVersionUID = -4210609503909770492L;

	        @Override
	        public Boolean call(Tuple2<Integer, Object[]> ss7Data) throws Exception
	        {
	            // Filter the records of which the sex in the second column is female.
	            Boolean isIsupData = (ss7Data._1() == 3);
	            return isIsupData;
	        }
	    }).map(
 		  new Function<Tuple2<Integer, Object[]>, Row>() {//int tempCount=0;
	 		    private static final long serialVersionUID = 8554347338412767649L;

	 			public Row call(Tuple2<Integer, Object[]> record) throws Exception {		    	
	 		      return RowFactory.create(record._2());
	 		    }
	 		  });
	 
	}

	public String printTimeDiffLogger() {
		long millis = System.currentTimeMillis() - startTime;
		
		return String.format(" dataCount :"+ dataCount + "time taken: %d min, %d sec", 
			    TimeUnit.MILLISECONDS.toMinutes(millis),
			    TimeUnit.MILLISECONDS.toSeconds(millis) - 
			    TimeUnit.MINUTES.toSeconds(TimeUnit.MILLISECONDS.toMinutes(millis))
			);
		
	}

	@Override
	public JavaRDD<Object[]> getData() {
		// TODO Auto-generated method stub
		return null;
	}



}
